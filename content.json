{"pages":[{"title":"About","text":"","link":"/about/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Python闭包和go闭包","text":"最近学习golang，发现golang里面也有闭包的概念，这里和python比较一下，并有些新的体会，记下来 1.Python 闭包闭包：引用了自由变量的函数，从下面这个例子看出，decorator的返回值_wrap引用了两个自由变量f和cache通过closure可以一探究竟，输出结果可以看出closure包含两个元素，一个是function，另外一个是cache， 123456789101112131415161718192021222324252627def decorator(f): cache = [] def _wrap(*args, **kwargs): cache.append(1) print(cache) f(*args, **kwargs) return _wrap# @decoratordef foo(): print(\"foo\")if __name__ == \"__main__\": a = decorator(foo) a() print(a.__closure__) print(a.__closure__[1].cell_contents) print(a.__closure__[0].cell_contents)# output#foo#(&lt;cell at 0x1094cd198: list object at 0x10acea248&gt;, &lt;cell at 0x1094cd4c8: function object at 0x107879268&gt;)#&lt;function foo at 0x107879268&gt;#[1] 之前写过一篇对于闭包中引用变量的生命周期，这次做了个实验，先看结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def decorator(f): cache = [] def _wrap(*args, **kwargs): cache.append(1) print(cache) f(*args, **kwargs) return _wrap# @decoratordef foo(): print(\"foo\")if __name__ == \"__main__\": a = decorator(foo) a() a() print(a.__closure__) print(a.__closure__[1].cell_contents) print(a.__closure__[0].cell_contents) b = decorator(foo) b() b() print(b.__closure__) print(b.__closure__[1].cell_contents) print(b.__closure__[0].cell_contents)# output# [1]# foo# [1, 1]# foo# (&lt;cell at 0x10eb34198: list object at 0x11a657248&gt;, &lt;cell at 0x10eb344c8: function object at 0x10cee0268&gt;)# &lt;function foo at 0x10cee0268&gt;# [1, 1]# [1]# foo# [1, 1]# foo# (&lt;cell at 0x10eb34a98: list object at 0x112abb188&gt;, &lt;cell at 0x10eb34af8: function object at 0x10cee0268&gt;)# &lt;function foo at 0x10cee0268&gt;# [1, 1] 可以看到闭包的自由变量的作用域对于每个函数是独立的，即a和b对于f和cache的引用是独立的 2.Go闭包go的闭包和python基本一样，看个例子 123456789101112131415161718192021222324252627282930313233343536package mainimport \"fmt\"func decorator(f func()) func(){ var cache []string cache = append(cache, \"1\") _wrap := func() { cache = append(cache, \"foo\") fmt.Printf(\"%s\\n\", cache) f() } return _wrap}func foo() { fmt.Println(\"foo\")}func main() { a := decorator(foo) a() a() b := decorator(foo) b() b()}// output// [1 foo]// foo// [1 foo foo]// foo// [1 foo]// foo// [1 foo foo]//foo 可以看到对于a和b对于自用变量的引用也是独立的，互不影响","link":"/2019/03/22/decorator_2/"},{"title":"关于go语言的介绍-《Go In Action》-Ch1","text":"1.1 Go 解决现代编程的难题1.1.1 开发速度更加智能的编译器，简化依赖算法，编译速度更快，只会关注直接被引用的库编译器提供类型检查 1.1.2 并发提供并发支持，goroutine比线程更轻量级的并发，内置channel，在不同goroutine之间通信 1.goroutinegoroutine是可以和其他goroutine并行执行的函数，使用的内存更少。运行时会自动的配置一组逻辑处理器执行goroutine，每个逻辑处理器绑定到一个操作系统线程上，程序实行效率更高1234func log(msg string) { ....}go log(\"blablabla\") 2.channelchannel是可以让goroutine之间进行安全通信的工具，避免共享内存访问的问题，保证同一时刻只会有一个goroutine修改数据, 但是channel并不提供跨goroutine的数据访问保护机制，如果传输的是副本，那么每个goroutine都持有一个副本，各自对副本修改是安全的。但是如果传输的诗指针时，还是需要额外的同步动作 1.1.3 Go语言的类型系统无继承，使用组合设计模式，具有接口机制对行为进行建模 1. 类型简单内置简单类型，支持自定义类型，使用组合来支持扩展 2.Go接口对一组行为建模一个类型实现了一个接口的所有方法，那么这个类型的实例就可以存储在这个接口类型的实例中，不需要额外声明Go语言的整个网络库都是用了io.Reader接口，这样可以将程序的功能和不同的网络实现分离，任何实现了open方法的类型，都实现了io.Reader接口1234//io.Readertype Reader interface{ Read(p []byte) (n int, err error) } 1.1.3 内存管理Golang提供GC 1.2 Hello Go12345package mainimport \"fmt\"func main() { fmt.Println(\"Hello World\")} Go Playground是提供在线编辑运行Go的网站","link":"/2019/04/10/go_intro/"},{"title":"打包和工具链-《Go In Action》-Ch3","text":"3.1 包对于所有的.go文件，都应该在首行声明自己所属的包，每个包在一个单独的目录中，不能把多个包放在一个目录中，也不能把相同的包拆分到不同的目，同一个包下的所有.go文件必须声明同一个包名 包命名惯例，简洁，顾名思义 main包是特殊的包，编译程序会将带有main包声明的包编译为二进制可执行文件，main包中一定要有main函数，否则不可以创建可执行文件 3.2 导入导入包查找顺序 GOROOT -》 GOPATH，一旦编译器找到一个满足的包就会停止查找，也可以远程导入包，通过go get 将GitHub的包下载到本地 12345import ( \"fmt\" \"strings\" \"github.com/spf13/viper\") 如果包重名，可以使用命名导入，将导入的包命名为新名字 1234567891011package mainimport ( \"fmt\" myfmt \"mylib/fmt\")func main() { fmt.Println(\"Standard Library\") myfmt.Println(\"mylib/fmt\")} 如果导入了不在代码使用的包，会导致编译失败，可以使用下划线来重命名不适用的包 3.3 函数init每个包可以包含任意多个init函数，在main之前被调用，init函数用在设置包、初始化变量或者其他要在程序运行前优先完成的引导工作，可以使用下划线来重命名不适用的包以数据库驱动为例，sql包在编译时并不知道要注册哪些驱动，如果我们要使用数据库连接，就需要用init函数将驱动注册到mysql上 123456789package postgresimport ( \"database/sql\")func init() { sql.Register(\"postgres\", new(PostgresDriver))} 在使用这个新的数据库驱动时，需要使用空白标识符导入包12345678910package mainimport (\"database/sql\"_ \"github.com/goinaction/code/chapter3/dbdriver/postgres\")func main() {sql.Open(\"postgres\", \"mydb\")} 3.4 使用Go的工具go build 执行编译参数为空时 默认编译当前目录参数可以为文件名参数可以为/… 会编译目录下的所有包 go clean 执行清理，会删除可执行文件go run 先构建再执行 3.5 进一步介绍Go开发工具go vet 检测代码常见错误 printf类型匹配错误参数 方法签名错误 错误的结构标签 没有指定字段名的结构字面量go fmt 格式化代码go doc 打印文档godoc 浏览器打开文档 godoc -http:6000 3.7 依赖管理没有实际管理过一个大工程，这里看的稀里糊涂的，暂时不表了，等搞清楚再补 3.8 小结Go语言中包是组织代码的基本单位环境变量GOPATH决定了GO源代码在磁盘上被保存、编译和安装的位置可以为每个工程设置不同的GOPATH，以保持源代码和依赖的隔离go工具是在命令行上工作的最好工具开发人员可以使用go get获取别人的包并将其安装到自己的GOPATH指定的目录想要为别人创建包很见到，只要把源代码放到共有代码库，把那个遵守一些简单的规则就可以了GO语言在设计时将分享代码作为语言的核心特性和驱动力推荐使用依赖管理工具来管理依赖有很多社区开发的依赖管理工具，godep、vendor、gb","link":"/2019/04/12/go_pack_tool/"},{"title":"Python中的装饰器","text":"Python中的装饰器用的较为普遍，大致思路是在函数运行前后封装一些操作，以实现诸如如打印日志、统计运行时间、保存中间变量的效果。本文通过几个实例说明装饰器的基本用法和应用场景。 1.引子浏览网上各种解释装饰器的文章，提到最多的就是斐波那契数列的计算，这里先给出基础的计算斐波那契数列的函数： 12345def fib(n): if n &lt;= 2: return n - 1 else: return fib(n - 1) + fib(n - 2) 以上代码采用递归的方式计算第n个斐波那契数，其实这种递归计算方法会产生很多重复计算，我们将fib(5)的计算拆解开： 1234567 fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\fib(2) fib(1) 从上面图中可以看到，fib(3)计算了2次，fib(2)计算了3次，fib(1)计算了2次，如果能将递归过程中的中间变量存储起来，就可以节省出很多时间，这里用装饰器的方法存储这些中间变量，首先给出代码： 12345678910111213141516171819def cache(f): cache_dict = {} @wraps(f) def _cache(n): if n in cache_dict.keys(): return cache_dict[n] else: cache_dict[n] = f(n) return cache_dict[n] return _cache@cachedef fib(n): if n &lt;= 2: return n - 1 else: return fib(n - 1) + fib(n - 2) 我们在装饰器中定义了一个全局的dict，用来存储第i个斐波那契值，每次计算fib(i)之前先去dict中查看是否已经缓存改值，如果缓存了直接从dict中取，否则计算fib(i)并写入dict中。 以上就实现了通过装饰器缓存部分变量，达到减少重复计算的目的，下面我们来了解一下装饰器的运行机制，以及变量的生命周期。 2.装饰器原理剖析从上面斐波那契数列的例子可以看到，装饰器其实是一个接受函数做参数，返回值为函数的函数。笼统的可以概括成以下的形式： 123456789101112def decorator(f): def _wrap(args): do somthing result = f(args) do somthing return result retun _wrap@decoratordef foo(args): do somthing 返回的函数其实包括了要运行的函数，并在函数运行前后做了若干操作。那当我们调用foo(args)到底发生了什么呢？ 当显示的调用foo(args)时，可以认为先执行了装饰器函数decorator(f)，装饰器函数返回了函数_wrap(args), 整体的调用顺序即是decorator(f)(args)，为了验证这个的结论，我们将上面斐波那契数列的例子修改一下，执行下面的语句： 123456print fib(20)print cache(fib)(20)#output41814181 可以看到两种输出方式结果是一致的， 从而验证了对于装饰器调用顺序的结论。为了更好的理解装饰器的调用顺序，这里对引子中的例子进行修改，再增加一层装饰器，如下： 12345678910111213141516171819202122232425262728293031323334def cache(f): cache_dict = {\"test\": \"foo\"} @wraps(f) def _cache(n): if n in cache_dict.keys(): return cache_dict[n] else: cache_dict[n] = f(n) return cache_dict[n] return _cachedef record(f): @wraps(f) def _wrap(n): start_time = time.time() result = f(n) end_time = time.time() logger.info('f_name:%s, n:%s, cost_time:%s', f.__name__, n, end_time - start_time) return result return _wrap@record@cachedef fib(n): if n &lt;= 2: return n - 1 else: return fib(n - 1) + fib(n - 2) 可以看到增加了record装饰器，作用是记录函数运行时间，先调用一下fib(20),看看结果: 123456782017-12-05 21:03:12,115 [140735241039872] - [decorate_learn.py 32] INFO n:2, cost_time:9.53674316406e-072017-12-05 21:03:12,115 [140735241039872] - [decorate_learn.py 32] INFO n:1, cost_time:3.09944152832e-062017-12-05 21:03:12,115 [140735241039872] - [decorate_learn.py 32] INFO n:3, cost_time:0.0004069805145262017-12-05 21:03:12,115 [140735241039872] - [decorate_learn.py 32] INFO n:2, cost_time:2.14576721191e-062017-12-05 21:03:12,116 [140735241039872] - [decorate_learn.py 32] INFO n:4, cost_time:0.0007228851318362017-12-05 21:03:12,116 [140735241039872] - [decorate_learn.py 32] INFO n:3, cost_time:3.09944152832e-062017-12-05 21:03:12,116 [140735241039872] - [decorate_learn.py 32] INFO n:5, cost_time:0.001335144042973 可以看到每次调用fib(n)函数的时间都被打印出来，如上面对装饰器调用顺序的结论，这里同样跑一下record(cache(fib))(5),得到如下结果： 1234567892017-12-05 21:09:35,869 [140735241039872] - [decorate_learn.py 32] INFO n:2, cost_time:2.86102294922e-062017-12-05 21:09:35,869 [140735241039872] - [decorate_learn.py 32] INFO n:1, cost_time:3.09944152832e-062017-12-05 21:09:35,869 [140735241039872] - [decorate_learn.py 32] INFO n:3, cost_time:0.0004301071166992017-12-05 21:09:35,869 [140735241039872] - [decorate_learn.py 32] INFO n:2, cost_time:1.90734863281e-062017-12-05 21:09:35,870 [140735241039872] - [decorate_learn.py 32] INFO n:4, cost_time:0.0006570816040042017-12-05 21:09:35,870 [140735241039872] - [decorate_learn.py 32] INFO n:3, cost_time:2.14576721191e-062017-12-05 21:09:35,870 [140735241039872] - [decorate_learn.py 32] INFO n:5, cost_time:0.000828027725222017-12-05 21:09:35,870 [140735241039872] - [decorate_learn.py 32] INFO n:5, cost_time:0.0009081363677983 以上研究了装饰器调用函数的流程，下面我们看下装饰器中变量的生命周期。注意到在斐波那契数列的例子中，定义了cache_dict字典，那该字典何时被创建，何时被销毁呢，为此我们做以下实验： 1234567891011import sysimport decorate_learnfor i in range(5): decorate_learn.fib(i + 1)reload(decorate_learn)for i in range(5): decorate_learn.fib(i + 1) 装饰器也稍作改变，每次调用的时候打印cache_dict12345678910111213def cache(f): cache_dict = {\"test\": \"foo\"} @wraps(f) def _cache(n): logger.info('n:%s,cache_dict:%s', n, cache_dict) if n in cache_dict.keys(): return cache_dict[n] else: cache_dict[n] = f(n) return cache_dict[n] return _cache 之所以这么做，是因为没有找到太好能够显示变量创建销毁的方法，所以每次调用装饰器的时候打印该变量，看下改变量的内容是否有被清空重建，看下输出日志： 123456789101112131415161718192021222017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:1,cache_dict:{&apos;test&apos;: &apos;foo&apos;}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:2,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:3,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:2,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:1,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:4,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:3,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:2,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:5,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1, 4: 2}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:4,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1, 4: 2}2017-12-06 09:45:07,733 [140735241039872] - [decorate_learn.py 16] INFO n:3,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1, 4: 2}2017-12-06 09:45:07,734 [140735241039872] - [decorate_learn.py 16] INFO n:1,cache_dict:{&apos;test&apos;: &apos;foo&apos;}2017-12-06 09:45:07,734 [140735241039872] - [decorate_learn.py 16] INFO n:2,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0}2017-12-06 09:45:07,735 [140735241039872] - [decorate_learn.py 16] INFO n:3,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1}2017-12-06 09:45:07,735 [140735241039872] - [decorate_learn.py 16] INFO n:2,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1}2017-12-06 09:45:07,735 [140735241039872] - [decorate_learn.py 16] INFO n:1,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1}2017-12-06 09:45:07,735 [140735241039872] - [decorate_learn.py 16] INFO n:4,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1}2017-12-06 09:45:07,736 [140735241039872] - [decorate_learn.py 16] INFO n:3,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1}2017-12-06 09:45:07,738 [140735241039872] - [decorate_learn.py 16] INFO n:2,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1}2017-12-06 09:45:07,738 [140735241039872] - [decorate_learn.py 16] INFO n:5,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1, 4: 2}2017-12-06 09:45:07,739 [140735241039872] - [decorate_learn.py 16] INFO n:4,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1, 4: 2}2017-12-06 09:45:07,739 [140735241039872] - [decorate_learn.py 16] INFO n:3,cache_dict:{&apos;test&apos;: &apos;foo&apos;, 1: 0, 2: 1, 3: 1, 4: 2} 从日志的输出可以看到，一次程序执行过程中，装饰器中dict是和fib函数同时存在的，只有当主程序退出时，dict才会销毁。以上我们研究了装饰器的写法和一些简单原理，下面给出一种使用类写装饰器的方法。 3.装饰器的另一种写法装饰器除了函数式的写法，还可以封装成类，并重写call方法即可，还是以菲波那切数列为例，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import timefrom functools import wrapsclass cache(object): def __init__(self): self.cache_dict = {} def __call__(self, f): @wraps(f) def _wrap(n): print self.cache_dict if n in self.cache_dict.keys(): return self.cache_dict[n] else: self.cache_dict[n] = f(n) return self.cache_dict[n] return _wrapclass record(object): def __init__(self): pass def __call__(self, f): @wraps(f) def _wrap(n): start_time = time.time() result = f(n) end_time = time.time() print 'f_name:%s, n:%s, cost_time:%s' % (f.__name__, n, end_time - start_time) return result return _wrap@record()@cache()def fib(n): if n &lt;= 2: return n - 1 else: return fib(n - 1) + fib(n - 2)@record()@cache()def foo(n): print \"foo\"# print fib(1)print fib(20)foo(1) 代码里需要解释的一点是我们引入了functools.wraps，目的是保持函数的类型一致。 12345678910111213#with wrapsfib(1)print fib.__name__#out#fib#without wrapsfib(1)print fib.__name__#out#_wrap 从上面可以看到，加了wrap可以让函数保持原有的名字 总结以上简单介绍了装饰器的实现方法和一些自己的小探究，笔而简之,以备阙忘。","link":"/2017/12/05/decorator/"},{"title":"并发 -《Go In Action》-Ch6","text":"每个goroutine是一个独立的工作单元，这个单元会被调度到可用的逻辑处理器上执行。Go运行时通过调度器管理goroutine，为其分配执行时间。调度器在操作系统之上，将操作系统的线程和语言运行时的逻辑处理器绑定，并在逻辑处理器上运行goroutine。Go语言通过在goroutine之间传递数据来通信，而不是对数据加锁来实现同步访问。 6.1 并行和并发 并发是让不同的代码片段同时在不同的物理处理器上执行，并发是指同时管理很多事情。每当创建一个goroutine并准备运行，goroutine被分配到调度器的全局队列中，调度器会给goroutine分配一个逻辑处理器，将goroutine放到逻辑处理器对应的本地队列中。 6.2 goroutine下面这个程序展示了逻辑处理器是如何调度goroutine的，runtime.GOMAXPROCS(1)只允许程序使用一个逻辑处理器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package mainimport ( \"fmt\" \"runtime\" \"sync\")// wg is used to wait for the program to finish.var wg sync.WaitGroup// main is the entry point for all Go programs.func main() { // Allocate 1 logical processors for the scheduler to use. runtime.GOMAXPROCS(1) // Add a count of two, one for each goroutine. wg.Add(2) // Create two goroutines. fmt.Println(\"Create Goroutines\") go printPrime(\"A\") go printPrime(\"B\") // Wait for the goroutines to finish. fmt.Println(\"Waiting To Finish\") wg.Wait() fmt.Println(\"Terminating Program\")}// printPrime displays prime numbers for the first 5000 numbers.func printPrime(prefix string) { // Schedule the call to Done to tell main we are done. defer wg.Done()next: for outer := 2; outer &lt; 5000; outer++ { for inner := 2; inner &lt; outer; inner++ { if outer%inner == 0 { continue next } } fmt.Printf(\"%s:%d\\n\", prefix, outer) } fmt.Println(\"Completed\", prefix)}// output// Create Goroutines// Waiting To Finish// B:2// B:3// ...// B:4583// B:4591// A:3 ** 切换 goroutine// A:5// ...// A:4561// A:4567// B:4603 ** 切换 goroutine// B:4621// ...// Completed B// A:4457 ** 切换 goroutine// A:4463// ...// A:4993// A:4999// Completed A// Terminating Program 可以看到goroutine A和B是交替运行的，因为只有一个逻辑处理器。调度过程可以用下图表示: 6.3 竞争状态多个goroutine同时对一个共享资源进行读和写，容易进入相互竞争的状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package mainimport ( \"fmt\" \"runtime\" \"sync\")var ( // counter is a variable incremented by all goroutines. counter int // wg is used to wait for the program to finish. wg sync.WaitGroup)// main is the entry point for all Go programs.func main() { // Add a count of two, one for each goroutine. wg.Add(2) // Create two goroutines. go incCounter(1) go incCounter(2) // Wait for the goroutines to finish. wg.Wait() fmt.Println(\"Final Counter:\", counter)}// incCounter increments the package level counter variable.func incCounter(id int) { // Schedule the call to Done to tell main we are done. defer wg.Done() for count := 0; count &lt; 2; count++ { // Capture the value of Counter. value := counter // Yield the thread and be placed back in queue. runtime.Gosched() // Increment our local value of Counter. value++ // Store the value back into Counter. counter = value }} 最后counter的值有可能是2，可以用下面这个图描述下过程 可以用go build -race检测代码里的竞争状态 12345678910111213141516171819go build -race // 用竞争检测器标志来编译程序./example // 运行程序==================WARNING: DATA RACEWrite by goroutine 5:main.incCounter()/example/main.go:49 +0x96Previous read by goroutine 6:main.incCounter()/example/main.go:40 +0x66Goroutine 5 (running) created at:main.main()/example/main.go:25 +0x5cGoroutine 6 (running) created at:main.main()/example/main.go:26 +0x73==================Final Counter: 2Found 1 data race(s) 6.4 锁住共享资源可以使用原子函数和互斥锁解决共享资源的问题 6.4.1 原子函数原子函数能够以很底层的加锁机制来同步访问整形变量和指针，atomic包提供了一些原子操作，如AddInt64，这个函数会同步整型类型的的加法LoadInt64和StoreInt64，这两个函数提供了一种安全的读写一个整型值的方式。1234var count int64atomic.AddInt64(&amp;counter, 1)atomic.LoadInt64(&amp;cunter)atomic.StoreInt64(&amp;count, 1) 6.4.2 互斥锁互斥锁用于在代码上创建一个临界区，保证同一时间只有一个goroutine 可以执行这个临界区代码1234mutex sync.Mutexmutex.Lock()...mutex.Unlock() 6.5 通道可以使用make来创建通道123456789// 无缓冲的整型通道unbuffered := make(chan int)// 有缓冲的字符串通道buffered := make(chan string, 10)// 向通道发送值buffered &lt;- \"Gopher\"// 从通道里接收值value := &lt;-buffered 无缓冲的通道（unbuffered channel）是指在接收前没有能力保存任何值的通道。这种类型的通道要求发送goroutine 和接收goroutine 同时准备好，才能完成发送和接收操作。如果两个goroutine没有同时准备好，通道会导致先执行发送或接收操作的goroutine 阻塞等待。这种对通道进行发送和接收的交互行为本身就是同步的。其中任意一个操作都无法离开另一个操作单独存在。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// This sample program demonstrates how to use an unbuffered// channel to simulate a relay race between four goroutines.package mainimport ( \"fmt\" \"sync\" \"time\")// wg is used to wait for the program to finish.var wg sync.WaitGroup// main is the entry point for all Go programs.func main() { // Create an unbuffered channel. baton := make(chan int) // Add a count of one for the last runner. wg.Add(1) // First runner to his mark. go Runner(baton) // Start the race. baton &lt;- 1 // Wait for the race to finish. wg.Wait()}// Runner simulates a person running in the relay race.func Runner(baton chan int) { var newRunner int // Wait to receive the baton. runner := &lt;-baton // Start running around the track. fmt.Printf(\"Runner %d Running With Baton\\n\", runner) // New runner to the line. if runner != 4 { newRunner = runner + 1 fmt.Printf(\"Runner %d To The Line\\n\", newRunner) go Runner(baton) } // Running around the track. time.Sleep(100 * time.Millisecond) // Is the race over. if runner == 4 { fmt.Printf(\"Runner %d Finished, Race Over\\n\", runner) wg.Done() return } // Exchange the baton for the next runner. fmt.Printf(\"Runner %d Exchange With Runner %d\\n\", runner, newRunner) baton &lt;- newRunner} 有缓冲的通道（buffered channel）是一种在被接收前能存储一个或者多个值的通道。这种类型的通道并不强制要求goroutine 之间必须同时完成发送和接收。通道会阻塞发送和接收动作的条件也会不同。只有在通道中没有要接收的值时，接收动作才会阻塞。只有在通道没有可用缓冲区容纳被发送的值时，发送动作才会阻塞。这导致有缓冲的通道和无缓冲的通道之间的一个很大的不同：无缓冲的通道保证进行发送和接收的goroutine 会在同一时间进行数据交换；有缓冲的通道没有这种保证。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// This sample program demonstrates how to use a buffered// channel to work on multiple tasks with a predefined number// of goroutines.package mainimport ( \"fmt\" \"math/rand\" \"sync\" \"time\")const ( numberGoroutines = 4 // Number of goroutines to use. taskLoad = 10 // Amount of work to process.)// wg is used to wait for the program to finish.var wg sync.WaitGroup// init is called to initialize the package by the// Go runtime prior to any other code being executed.func init() { // Seed the random number generator. rand.Seed(time.Now().Unix())}// main is the entry point for all Go programs.func main() { // Create a buffered channel to manage the task load. tasks := make(chan string, taskLoad) // Launch goroutines to handle the work. wg.Add(numberGoroutines) for gr := 1; gr &lt;= numberGoroutines; gr++ { go worker(tasks, gr) } // Add a bunch of work to get done. for post := 1; post &lt;= taskLoad; post++ { tasks &lt;- fmt.Sprintf(\"Task : %d\", post) } // Close the channel so the goroutines will quit // when all the work is done. close(tasks) // Wait for all the work to get done. wg.Wait()}// worker is launched as a goroutine to process work from// the buffered channel.func worker(tasks chan string, worker int) { // Report that we just returned. defer wg.Done() for { // Wait for work to be assigned. task, ok := &lt;-tasks if !ok { // This means the channel is empty and closed. fmt.Printf(\"Worker: %d : Shutting Down\\n\", worker) return } // Display we are starting the work. fmt.Printf(\"Worker: %d : Started %s\\n\", worker, task) // Randomly wait to simulate work time. sleep := rand.Int63n(100) time.Sleep(time.Duration(sleep) * time.Millisecond) // Display we finished the work. fmt.Printf(\"Worker: %d : Completed %s\\n\", worker, task) }} 上面代码需要注意的是close(tasks)，关闭通道后，goroutine依旧可以从通道接收数据，但是不能再向通道里发送数据。 6.6 小结并发是指goroutine运行的时候是相互独立的使用关键字go创建goroutine来运行函数goroutine在逻辑处理器上执行，逻辑处理器具有独立的系统线程和运行队列竞争状态是指两个或者多个goroutine试图访问同一个资源原子函数和互斥锁提供了一种防止出现竞争状态的办法通道提供了一种在两个goroutine之间共享数据的简单方法无缓冲的通道保证同时交换数据，而有缓冲的通道不做这种保证","link":"/2019/04/13/go_goroutine/"},{"title":"快速开始一个GO程序-《Go In Action》-Ch2","text":"一上来接来个大程序，新手能接得住么这个程序从不同的数据源拉取数据，将数据内容与一组搜索项做对比，然后将匹配的内容显示在终端窗口。这个程序会读取文本文件，进行网络调用，解码XML 和JSON 成为结构化类型数据，并且利用Go 语言的并发机制保证这些操作的速度source code 2.1 程序架构1234567891011- sample - data data.json -- 包含一组数据源 - matchers rss.go -- 搜索 rss 源的匹配器 - search default.go -- 搜索数据用的默认匹配器 feed.go -- 用于读取 json 数据文件 match.go -- 用于支持不同匹配器的接口 search.go -- 执行搜索的主控制逻辑 main.go -- 程序的入口 2.2 main 包123456789101112131415161718192021package mainimport ( \"log\" \"os\" _ \"github.com/goinaction/code/chapter2/sample/matchers\" \"github.com/goinaction/code/chapter2/sample/search\")// init is called prior to main.func init() { // Change the device for logging to stdout. log.SetOutput(os.Stdout)}// main is the entry point for the program.func main() { // Perform the search for the specified term. search.Run(\"president\")} 有以下几点需要注意： main()是程序的入口，没有main函数，构建程序不会生成可执行文件 一个包定义一组编译通过的代码，包的名字类似命名空间，可以用来直接访问包内生命的标识符， 可以报不同包中定义的同名标识符区别开 下划线开头的包，是为了进行包的初始化操作，GO不允许声明导入包却不使用，下划线让编译器接受这种到日，并且调用对应包内所有文件代码里定义的init函数，init函数的执行在main函数之前 2.3 search 包serach 包包含了程序使用的框架和业务逻辑 2.3.1 serach.goserach文件先获取数据源，然后对每个数据源获取的数据进行匹配，每一个匹配启用一个goroutine。使用sync.WaitGroup控制任务是否完成。sync.WaitGroup是一个计数信号量，主要有三个方法Add、Done和Wait，每增加一个任务就Add一次，每完成一个任务就Done一次，调用Wait的时候程序会阻塞，直到所有任务完成。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package search//从标准库导入代码时，只需要给出要导入包的包名，//编译器查找包时，总是会到GOROOT和GOPATH环境变量引用的位置去查找import ( \"log\" \"sync\")// A map of registered matchers for searching.// 小写字母标识，标识包内变量，不导出 or 不公开var matchers = make(map[string]Matcher)// Run performs the search logic.func Run(searchTerm string) { // Retrieve the list of feeds to search through. feeds, err := RetrieveFeeds() if err != nil { log.Fatal(err) } // Create an unbuffered channel to receive match results to display. results := make(chan *Result) // Setup a wait group so we can process all the feeds. var waitGroup sync.WaitGroup // Set the number of goroutines we need to wait for while // they process the individual feeds. waitGroup.Add(len(feeds)) // Launch a goroutine for each feed to find the results. for _, feed := range feeds { // Retrieve a matcher for the search. matcher, exists := matchers[feed.Type] if !exists { matcher = matchers[\"default\"] } // Launch the goroutine to perform the search. go func(matcher Matcher, feed *Feed) { Match(matcher, feed, searchTerm, results) waitGroup.Done() }(matcher, feed) } // Launch a goroutine to monitor when all the work is done. go func() { // Wait for everything to be processed. waitGroup.Wait() // Close the channel to signal to the Display // function that we can exit the program. close(results) }() // Start displaying results as they are available and // return after the final result is displayed. Display(results)}// Register is called to register a matcher for use by the program.func Register(feedType string, matcher Matcher) { if _, exists := matchers[feedType]; exists { log.Fatalln(feedType, \"Matcher already registered\") } log.Println(\"Register\", feedType, \"matcher\") matchers[feedType] = matcher} 对于上面代码，有以下问题需要明确下： feeds, err := RetrieveFeeds() 这种一个函数返回两个值，第一个参数返回值，第二个返回错误信息，是GO中常用的模式 声明运算符（:=），这个运算符在声明变量的同时，给变量赋值 feeds 是一个切片，可以理解为Go里面的动态数组，是一种引用类型 results是一个无缓冲的channel，和map、slice一样，都是引用类型，channel内置同步机制，从而保证通信安全 Go中，如果main函数返回，整个程序也就终止了，终止时，会关闭所有之前启动而且还在运行的goroutine for range对feeds切片做迭代，和python里面的 for in一样的道理，每次迭代会返回两个值（index，value），value是一个副本，下划线_是一个占位符 使用go关键字启动一个goroutine，并对这个goroutine做并发调度。上面程序中go启动了一个匿名函数作为goroutine 在Go语言中，所有的变量都是以值的方式传递。所以想要修改真正的值，可以传递指针 Go语言支持闭包，匿名函数中访问searchTerm、results就是通过闭包的形势访问的。注意matcher、feed这两个变量并没有使用闭包的形式访问 2.3.2 feed.gofeed会从本地的data/data.json中读取Json数据，并将数据反序列化为feed切片,defer会安排随后的函数调用在函数返回时才执行, 使用defer可以缩短打开文件和关闭文件的代码间隔 123456789101112131415161718192021222324252627282930313233343536package searchimport ( \"encoding/json\" \"os\")const dataFile = \"data/data.json\"// Feed contains information we need to process a feed.type Feed struct { Name string `json:\"site\"` URI string `json:\"link\"` Type string `json:\"type\"`}// RetrieveFeeds reads and unmarshals the feed data file.func RetrieveFeeds() ([]*Feed, error) { // Open the file. file, err := os.Open(dataFile) if err != nil { return nil, err } // Schedule the file to be closed once // the function returns. defer file.Close() // Decode the file into a slice of pointers // to Feed values. var feeds []*Feed err = json.NewDecoder(file).Decode(&amp;feeds) // We don't need to check for errors, the caller can do this. return feeds, err} 2.3.3 match.go/default.go123456789101112131415package search// defaultMatcher implements the default matcher.type defaultMatcher struct{}// init registers the default matcher with the program.func init() { var matcher defaultMatcher Register(\"default\", matcher)}// Search implements the behavior for the default matcher.func (m defaultMatcher) Search(feed *Feed, searchTerm string) ([]*Result, error) { return nil, nil} func (m defaultMatcher) Search 意味着search和defaultMatcher的值绑定在了一起，我们可以使用defaultMatcher 类型的值或者指向这个类型值的指针来调用Search 方法。无论我们是使用接收者类型的值来调用这个方，还是使用接收者类型值的指针来调用这个方法，编译器都会正确地引用或者解引用对应的值，作为接收者传递给Search 方法 123456789101112// 方法声明为使用defaultMatcher 类型的值作为接收者func (m defaultMatcher) Search(feed *Feed, searchTerm string)// 声明一个指向defaultMatcher 类型值的指针dm := new(defaultMatch)// 编译器会解开dm 指针的引用，使用对应的值调用方法dm.Search(feed, \"test\")// 方法声明为使用指向defaultMatcher 类型值的指针作为接收者func (m *defaultMatcher) Search(feed *Feed, searchTerm string)// 声明一个defaultMatcher 类型的值var dm defaultMatch// 编译器会自动生成指针引用dm 值，使用指针调用方法dm.Search(feed, \"test\") 与直接通过值或者指针调用方法不同，如果通过接口类型的值调用方法，规则有很大不同，如代码清单2-38 所示。使用指针作为接收者声明的方法，只能在接口类型的值是一个指针的时候被调用。使用值作为接收者声明的方法，在接口类型的值为值或者指针时，都可以被调用。12345678910111213141516// 方法声明为使用指向defaultMatcher 类型值的指针作为接收者func (m *defaultMatcher) Search(feed *Feed, searchTerm string)// 通过interface 类型的值来调用方法var dm defaultMatchervar matcher Matcher = dm // 将值赋值给接口类型matcher.Search(feed, \"test\") // 使用值来调用接口方法&gt; go buildcannot use dm (type defaultMatcher) as type Matcher in assignment// 方法声明为使用defaultMatcher 类型的值作为接收者func (m defaultMatcher) Search(feed *Feed, searchTerm string)// 通过interface 类型的值来调用方法var dm defaultMatchervar matcher Matcher = &amp;dm // 将指针赋值给接口类型matcher.Search(feed, \"test\") // 使用指针来调用接口方法&gt; go buildBuild Successful match创建不同类型的匹配器，Matcher其实是一个接口，对于每种匹配器又有不同的具体实现。下面的代码中，Matcher接口定义了一个Search方法，每个实现了Search方法的类型都实现了Matcher接口 12345678910111213141516171819202122232425262728293031323334353637383940414243package searchimport ( \"log\")// Result contains the result of a search.type Result struct { Field string Content string}// Matcher defines the behavior required by types that want// to implement a new search type.type Matcher interface { Search(feed *Feed, searchTerm string) ([]*Result, error)}// Match is launched as a goroutine for each individual feed to run// searches concurrently.func Match(matcher Matcher, feed *Feed, searchTerm string, results chan&lt;- *Result) { // Perform the search against the specified matcher. searchResults, err := matcher.Search(feed, searchTerm) if err != nil { log.Println(err) return } // Write the results to the channel. for _, result := range searchResults { results &lt;- result }}// Display writes results to the console window as they// are received by the individual goroutines.func Display(results chan *Result) { // The channel blocks until a result is written to the channel. // Once the channel is closed the for loop terminates. for result := range results { log.Printf(\"%s:\\n%s\\n\\n\", result.Field, result.Content) }} Display方法会迭代results这个channel，有数据时会打印，没数据时会阻塞，当main.go中的close(result)后，for range循环结束 注意到default.go有init函数，这个函数会在main中通过下划线导入包的时候执行，init的功能是初始化匹配器 2.4 RSS 匹配器rss.go篇幅过长，这里不贴代码了，其中有几个关注的点说下：在init中注册了一个rssMatcher，这个match和之前的defaultMatcher一样，绑定了Search方法，即实现了Matcher接口1234func init() { var matcher rssMatcher search.Register(\"rss\", matcher)} rss.go主要有两个方法retrieve和Search，retrieve负责抓取网略资源，search负责匹配，具体匹配方法这里不表了 2.5 小结 每个代码文件都属于一个包，而包名应该与代码文件所在的文件夹同名。 Go 语言提供了多种声明和初始化变量的方式。如果变量的值没有显式初始化，编译器会将变量初始化为零值。 使用指针可以在函数间或者goroutine 间共享数据。 通过启动goroutine 和使用通道完成并发和同步。 Go 语言提供了内置函数来支持Go 语言内部的数据结构。 标准库包含很多包，能做很多很有用的事情。 使用Go 接口可以编写通用的代码和框架。","link":"/2019/04/11/go_quick_prog/"},{"title":"数组、切片和映射-《Go In Action》-Ch4","text":"4.1 数组的内部实现和基础功能4.1.1 内部实现长度固定、内存连续分配、CPU数据缓存更久、容易计算索引，迭代速度快 4.2.1 声明和初始化声明需要类型和长度，长度一旦确定就不能改变1var array [5] int 声明变量时，会使用对应类型的零值对变量进行初始化可以使用字面变量声明数组1array := [5]int{10, 20, 30, 40, 50} 也可以使用…替代数组长度，Go会根据初始化时数组元素的数量来确定数组的长度1array := [...]int{10, 20, 30, 40, 50} 还可以给指定位置赋值确定值1array := [5]int{1: 10, 2: 20} 4.1.3 使用数组123456789101112131415161718192021222324252627282930313233343536373839array := [5]int{10, 20, 30, 40, 50}// 修改索引为2 的元素的值array[2] = 35// 声明包含5 个元素的指向整数的数组// 用整型指针初始化索引为0 和1 的数组元素array := [5]*int{0: new(int), 1: new(int)}// 为索引为0 和1 的元素赋值*array[0] = 10*array[1] = 20// 声明第一个包含5 个元素的字符串数组var array1 [5]string// 声明第二个包含5 个元素的字符串数组// 用颜色初始化数组array2 := [5]string{\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Pink\"}// 把array2 的值复制到array1array1 = array2// 声明第一个包含4 个元素的字符串数组var array1 [4]string// 声明第二个包含5 个元素的字符串数组// 使用颜色初始化数组array2 := [5]string{\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Pink\"}// 将array2 复制给array1array1 = array2// Compiler Error:// cannot use array2 (type [5]string) as type [4]string in assignment// 声明第一个包含3 个元素的指向字符串的指针数组var array1 [3]*string// 声明第二个包含3 个元素的指向字符串的指针数组// 使用字符串指针初始化这个数组array2 := [3]*string{new(string), new(string), new(string)}// 使用颜色为每个元素赋值*array2[0] = \"Red\"*array2[1] = \"Blue\"*array2[2] = \"Green\"// 将array2 复制给array1array1 = array2 4.1.4 多维数组1234567891011121314151617181920212223242526272829303132// 声明一个二维整型数组，两个维度分别存储4 个元素和2 个元素var array [4][2]int// 使用数组字面量来声明并初始化一个二维整型数组array := [4][2]int{{10, 11}, {20, 21}, {30, 31}, {40, 41}}// 声明并初始化外层数组中索引为1 个和3 的元素array := [4][2]int{1: {20, 21}, 3: {40, 41}}// 声明并初始化外层数组和内层数组的单个元素array := [4][2]int{1: {0: 20}, 3: {1: 41}}// 声明一个2×2 的二维整型数组var array [2][2]int// 设置每个元素的整型值array[0][0] = 10array[0][1] = 20array[1][0] = 30array[1][1] = 40// 声明两个不同的二维整型数组var array1 [2][2]intvar array2 [2][2]int// 为每个元素赋值array2[0][0] = 10array2[0][1] = 20array2[1][0] = 30array2[1][1] = 40// 将array2 的值复制给array1array1 = array2// 将 array1 的索引为1 的维度复制到一个同类型的新数组里var array3 [2]int = array1[1]// 将外层数组的索引为1、内层数组的索引为0 的整型值复制到新的整型变量里var value int = array1[1][0] 4.1.5 在函数间传递数组内存和性能上，传递数组是个很大的开销，因为总是值传递，需要拷贝，可以使用指针在函数间传递大数组，但是传递指针，函数会有改变指针指向的值的权限1234567var array [1e6]int// 将数组的地址传递给函数foofoo(&amp;array)// 函数foo 接受一个指向100 万个整型值的数组的指针func foo(array *[1e6]int) {...} 4.2 切片的内部实现和基础功能切片类似于动态数组，可以按需自动增长和缩小，通过内置append函数，可以高效增长切片，切片在内存中连续分配，可以索引、迭代 4.2.1 内部实现三个要素：指向底层数组的指针、切片访问元素的个数（即长度）和切片允许增长到的元素个数（即容量） 4.2.2 创建和初始化12345678910111213141516171819202122232425262728293031323334353637// 创建一个字符串切片// 其长度和容量都是5 个元素slice := make([]string, 5)// 创建一个整型切片// 分别指定长度和容量时，创建的切片，底层数组的长度是指定的容量，但是初始化后并不能访问所有的数组元素 // 这里不能访问最后两个元素slice := make([]int, 3, 5)// 容量小于长度的切片会在编译时报错// 创建一个整型切片// 使其长度大于容量slice := make([]int, 5, 3)// Compiler Error:// len larger than cap in make([]int)// 通过切片字面量来声明切片// 创建字符串切片// 其长度和容量都是5 个元素slice := []string{\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Pink\"}// 创建一个整型切片// 其长度和容量都是3 个元素slice := []int{10, 20, 30}// 使用索引声明切片// 创建字符串切片// 使用空字符串初始化第100 个元素slice := []string{99: \"\"}// 创建nil 整型切片// 数组指针为nil，长度和容量都是0var slice []int// 声明空切片// 数组包含0个元素，长度和容量都是0// 使用make 创建空的整型切片slice := make([]int, 0)// 使用切片字面量创建空的整型切片slice := []int{} 4.2.3 使用切片123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899// 创建一个整型切片// 其容量和长度都是5 个元素slice := []int{10, 20, 30, 40, 50}// 改变索引为1 的元素的值slice[1] = 25// 创建一个整型切片// 其长度和容量都是5 个元素slice := []int{10, 20, 30, 40, 50}// 创建一个新切片// 其长度为2 个元素，容量为4 个元素newSlice := slice[1:3]// 修改切片内容可能导致的结果// 创建一个整型切片// 其长度和容量都是5 个元素slice := []int{10, 20, 30, 40, 50}// 创建一个新切片// 其长度是2 个元素，容量是4 个元素newSlice := slice[1:3]// 修改newSlice 索引为1 的元素// 同时也修改了原来的slice 的索引为2 的元素newSlice[1] = 35// 表示索引越界的语言运行时错误// 创建一个整型切片// 其长度和容量都是5 个元素slice := []int{10, 20, 30, 40, 50}// 创建一个新切片// 其长度为2 个元素，容量为4 个元素newSlice := slice[1:3]// 修改newSlice 索引为3 的元素// 这个元素对于newSlice 来说并不存在newSlice[3] = 45// Runtime Exception:// panic: runtime error: index out of range// 切片增长// 创建一个整型切片// 其长度和容量都是5 个元素slice := []int{10, 20, 30, 40, 50}// 创建一个新切片// 其长度为2 个元素，容量为4 个元素newSlice := slice[1:3]// 使用原有的容量来分配一个新元素// 将新元素赋值为60// 注意此时 slice变为：{10, 20, 30, 60, 50}// append时如果容量有剩余，会在现有数组上增加元素，如果容量没有剩余，会创建一个新的数组，并想现有值复制到新的数组上// 当切片容量小于1000时，每次扩展成倍增加，一旦元素超过1000，容量银子会设为1.25，也就是每次增加25%newSlice = append(newSlice, 60)// 创建切片时的3个索引// 创建字符串切片// 其长度和容量都是5 个元素source := []string{\"Apple\", \"Orange\", \"Plum\", \"Banana\", \"Grape\"}// 将第三个元素切片，并限制容量// 其长度为1 个元素，容量为2 个元素slice := source[2:3:4]// 这比可用的容量大slice := source[2:3:6]// Runtime Error:// panic: runtime error: slice bounds out of range// 3个索引一旦长度和容量设置的不一样，新的切片和原始切片公用相同的底层数组，对新切片的append会影响到原始切片，很容发生莫名其妙的问题，// 此时可将新切片的容量设置为和长度一样，再执行append的时候，就会创建新的底层数组，从而和原始切片脱离关系，可以放心修改// 创建字符串切片// 其长度和容量都是5 个元素source := []string{\"Apple\", \"Orange\", \"Plum\", \"Banana\", \"Grape\"}// 对第三个元素做切片，并限制容量// 其长度和容量都是1 个元素slice := source[2:3:3]// 向slice 追加新字符串slice = append(slice, \"Kiwi\")// 将一个切片追加到另一个切片// ...运算符，可以将一个切片的所有元素追加到另一个切片里s1 := []int{1, 2}s2 := []int{3, 4}// 将两个切片追加在一起，并显示结果fmt.Printf(\"%v\\n\", append(s1, s2...))Output:[1 2 3 4]// 迭代切片for range// 关键字range 会返回两个值。第一个值是当前迭代到的索引位置，第二个值是该位置对应元素值的一份副本// 需要强调的是，range 创建了每个元素的副本，而不是直接返回对该元素的引用// 创建一个整型切片// 其长度和容量都是4 个元素slice := []int{10, 20, 30, 40}// 迭代每个元素，并显示值和地址for index, value := range slice { fmt.Printf(\"Value: %d Value-Addr: %X ElemAddr: %X\\n\", value, &amp;value, &amp;slice[index])}// Output:// Value: 10 Value-Addr: 10500168 ElemAddr: 1052E100// Value: 20 Value-Addr: 10500168 ElemAddr: 1052E104// Value: 30 Value-Addr: 10500168 ElemAddr: 1052E108// Value: 40 Value-Addr: 10500168 ElemAddr: 1052E10C 4.2.4 多维切片1234// 创建一个整型切片的切片slice := [][]int{{10}, {100, 200}}// 为第一个切片追加值为20 的元素slice[0] = append(slice[0], 20) 4.2.5 在函数间传递切片12345678910// 成本很低，在 64 位架构的机器上，一个切片需要24 字节的内存：指针字段需要8 字节，长度和容量字段分别需要8 字节。slice := make([]int, 1e6)// 将slice 传递到函数fooslice = foo(slice)// 函数foo 接收一个整型切片，并返回这个切片func foo(slice []int) []int { ... return slice} 4.3 映射的内部实现和基础功能4.3.1 内部实现桶 + 两个数组key转换成散列值，散列低位表示桶的序号，每个桶内有两个数组构成，第一个数组存储散列键的高发位置，第二个数组是一个字节数组，用于存储键值对，该字节数组先依次存储了这个桶里的所有键，之后依次存储了这个桶里的所有值。 4.3.2 创建和初始化映射的键可以是任何可以使用（==）比较的值，切片、函数以及包含切片的结构类型不能作为映射的键，因为他们包含引用语义12345678910// 创建一个映射，键的类型是string，值的类型是intdict := make(map[string]int)// 创建一个映射，键和值的类型都是string// 使用两个键值对初始化映射dict := map[string]string{\"Red\": \"#da1337\", \"Orange\": \"#e95a22\"}// 创建一个映射，使用字符串切片作为映射的键dict := map[[]string]int{}// Compiler Exception:// invalid map key type []string 4.3.3 使用映射123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 创建一个空映射，用来存储颜色以及颜色对应的十六进制代码colors := map[string]string{}// 将Red 的代码加入到映射colors[\"Red\"] = \"#da1337\"// 通过声明映射创建一个nil 映射, 可以通过声明一个未初始化的映射来创建一个值为nil 的映射（称为nil 映射）。nil 映射// 不能用于存储键值对，否则，会产生一个语言运行时错误var colors map[string]string// 将Red 的代码加入到映射colors[\"Red\"] = \"#da1337\"// Runtime Error:// panic: runtime error: assignment to entry in nil map// 从映射获取值并判断键是否存在// 获取键Blue 对应的值value, exists := colors[\"Blue\"]// 这个键存在吗？if exists { fmt.Println(value)}// 从映射获取值，并通过该值是否为零值来判断键是否存在// 获取键Blue 对应的值value := colors[\"Blue\"]// 这个键存在吗？if value != \"\" { fmt.Println(value)}// 遍历for range// 创建一个映射，存储颜色以及颜色对应的十六进制代码colors := map[string]string{ \"AliceBlue\": \"#f0f8ff\", \"Coral\": \"#ff7F50\", \"DarkGray\": \"#a9a9a9\", \"ForestGreen\": \"#228b22\",}// 显示映射里的所有颜色for key, value := range colors { fmt.Printf(\"Key: %s Value: %s\\n\", key, value)}// 删除键为Coral 的键值对delete(colors, \"Coral\")// 显示映射里的所有颜色for key, value := range colors { fmt.Printf(\"Key: %s Value: %s\\n\", key, value)} 4.3.4 在函数间传递映射在函数间传递映射并不会制造出该映射的副本，实际上，当传递映射给一个函数，并对这个映射做了修改时，所有对这个映射的引用都会察觉到这个修改，和切片类似1234567891011121314151617181920212223242526272829303132func main() {// 创建一个映射，存储颜色以及颜色对应的十六进制代码colors := map[string]string{ \"AliceBlue\": \"#f0f8ff\", \"Coral\": \"#ff7F50\", \"DarkGray\": \"#a9a9a9\", \"ForestGreen\": \"#228b22\",}// 显示映射里的所有颜色for key, value := range colors { fmt.Printf(\"Key: %s Value: %s\\n\", key, value)}// 调用函数来移除指定的键removeColor(colors, \"Coral\")// 显示映射里的所有颜色for key, value := range colors { fmt.Printf(\"Key: %s Value: %s\\n\", key, value)}}// removeColor 将指定映射里的键删除func removeColor(colors map[string]string, key string) { delete(colors, key)}// output// Key: AliceBlue Value: #F0F8FF// Key: Coral Value: #FF7F50// Key: DarkGray Value: #A9A9A9// Key: ForestGreen Value: #228B22// Key: AliceBlue Value: #F0F8FF// Key: DarkGray Value: #A9A9A9// Key: ForestGreen Value: #228B22 4.4 小结数组是构造切片和映射的基石Go语言里切片经常用来处理数据的集合，映射用来处理具有键值对结构的数据内置函数make可以创建切片和映射，并指定原始的长度和容量，也可以直接使用切片和映射字面量，或者使用字面量作为变量的初始值切片有容量限制，不过可以使用内置的append函数扩展容量映射的增长没有容量或者任何限制内置函数len可以用来获取切片或者映射的长度内置函数cap只能用于切片通过组合，可以创建多维数组和多维切片。也可以使用切片或者其他映射作为映射的值，但是切片不能用作映射的键将切片或者映射传递给函数的成本很小，并且不会复制底层的数据结构","link":"/2019/04/13/go_set/"},{"title":"Go语言的类型系统 -《Go In Action》-Ch5","text":"Go语言是一种静态类型的编程语言，编译器需要在编译时知道每个值的类型 5.1 用户定义类型声明一个新类型，即告诉编译器类型需要的内存大小和表示信息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 第一种声明方式type user struct { name string email string}// 使用结构类型声明变量，并初始化为其零值// 声明user 类型的变量var bill user// 使用结构字面量来声明一个结构类型的变量// 声明user 类型的变量，并初始化所有字段lisa := user{ name: \"Lisa\", email: \"lisa@email.com\", ext: 123, privileged: true,}// 不使用字段名，创建结构类型的值// 声明user 类型的变量lisa := user{\"Lisa\", \"lisa@email.com\", 123, true}// 使用其他结构类型声明字段// admin 需要一个user 类型作为管理者，并附加权限type admin struct { person user level string}// 使用结构字面量来创建字段的值// 声明admin 类型的变量fred := admin{ person: user{ name: \"Lisa\", email: \"lisa@email.com\", ext: 123, privileged: true, }, level: \"super\",}// 第二种声明方式// 基于int64 声明一个新类型// int64 和 Duration 是两种不同的类型，int64是Duration的基础类型，试图对ini64和Duration相互赋值将产生编译错误package maintype Duration int64func main() { var dur Duration dur = int64(1000)}// prog.go:7: cannot use int64(1000) (type int64) as type Duration// in assignment 5.2 方法方法能给用户定义的类型添加新的行为，方法实际上也是函数，只是在声明时，在关键字func和方法名之间的增加了一个参数，这个参数被称为接收者，将函数和接收者的类型绑在一起。 123456789101112131415161718192021222324252627282930313233343536373839404142// 这个示例程序展示如何声明// 并使用方法package mainimport (\"fmt\")// user 在程序里定义一个用户类型type user struct {name stringemail string}// notify 使用值接收者实现了一个方法func (u user) notify() {fmt.Printf(\"Sending User Email To %s&lt;%s&gt;\\n\",u.name,u.email)}// changeEmail 使用指针接收者实现了一个方法func (u *user) changeEmail(email string) {u.email = email}// main 是应用程序的入口func main() {// user 类型的值可以用来调用// 使用值接收者声明的方法bill := user{\"Bill\", \"bill@email.com\"}bill.notify()// 指向user 类型值的指针也可以用来调用// 使用值接收者声明的方法lisa := &amp;user{\"Lisa\", \"lisa@email.com\"}lisa.notify()// user 类型的值可以用来调用// 使用指针接收者声明的方法bill.changeEmail(\"bill@newdomain.com\")bill.notify() notify接受者是user值的一个副本，notify也可以使用指针调用，Go会在背后执行一个转换操作123// Go在代码背后的执行动作lisa := &amp;user{\"Lisa\", \"lisa@email.com\"}*(lisa).notify() 可以不到不管是使用值调用，还是使用指针调用，notify函数的接收者都是一个user的副本，对副本的修改并不会影响原来的值changeEmail恰恰相反，他的接受者是指针，这种情况函数对值进行的修改，会影响到原来的变量值，绑定指针类型的函数，也可以接受值的调用Go会在背后做如下优化1(&amp;bill).changeEmail(\"bill@newdomain.com\") 5.3 类型的本质一个类型在以参数在函数间传递或者作为接受者绑定方法时，需要根据类型的特点以及使用的方法，去决定是传指针还是传值 5.3.1 内置类型原始的，内置类型是语言提供的一组类型，诸如数值类型、字符串类型和布尔类型，对于这种类型的传递一般是传值，因为对这些值进行增加或者删除的时候，会创建一个新的值 5.3.2 引用类型非原始的，引用类型诸如切片、映射、通道、接口和函数类型，每个引用类型包含一组独特的字段，用于管理底层数据。不需要共享一个引用类型的值，可以通过赋值来传递一个引用类型的值的副本，本质上这就是在共享底层数据结构 5.3.3 结构类型结构类型有可能是原始的，也有可能是非原始的，需要遵守上面内置类型和引用类型的规范。是使用值接受者还是使用指针接受者，不应该由该方法是否修改了接受到的值来决定，应该基于该类型的本质。 5.4 接口多态是指代码可以根据类型的具体实现采取不同行为的能力，如果一个类型实现了某个接口，所有使用这个接口的地方，都可以支持这种类型的值。 5.4.1 标准库下面这个程序实现了类似于curl的基本功能，io.Copy的第一个参数是复制到的目标，这个参数是必须实现了io.Writer接口的值，os.Stdout实现了io.Writer。io.Copy的第二个参数接收一个io.Reader接口类型的值，表示数据流入的源，http.Response.Body实现了io.Reader接口1234567891011121314151617181920212223242526272829303132package mainimport ( \"fmt\" \"io\" \"net/http\" \"os\")// init is called before main.func init() { if len(os.Args) != 2 { fmt.Println(\"Usage: ./example2 &lt;url&gt;\") os.Exit(-1) }}// main is the entry point for the application.func main() { // Get a response from the web server. r, err := http.Get(os.Args[1]) if err != nil { fmt.Println(err) return } // Copies from the Body to Stdout. io.Copy(os.Stdout, r.Body) if err := r.Body.Close(); err != nil { fmt.Println(err) }} 5.4.2 实现接口是用来定义行为的类型。行为通过方法由用户定义的类型实现。用户定义的类型实现了某个接口类型声明的一组方法，那么这个用户定义的类型的值就可以赋给这个接口类型的值。这个赋值会把用户定义的类型的值存入接口类型的值。 对接口值方法的调用会执行接口值里存储的用户定义的类型的值的方法。将自定义类型赋值给接口分两种情况，自定义类型的值赋值给接口值和自定义类型指针赋值给接口值。下面两幅图展示了分别赋值给接口值后接口值的内存布局 接口值是两个字长度的数据结构，第一个字包含一个指向内部表的指针。内部表叫做iTable，包含了所存储的值的类型信息。iTable包含了已存储的值的类型信息和与这个值相关联的的一组方法。第二个字是一个指向所存储值的指针。 5.4.3 方法集方法集定义了接口的接受规则12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( \"fmt\")// notifier is an interface that defined notification// type behavior.type notifier interface { notify()}// user defines a user in the program.type user struct { name string email string}// notify implements a method with a pointer receiver.func (u *user) notify() { fmt.Printf(\"Sending user email to %s&lt;%s&gt;\\n\", u.name, u.email)}// main is the entry point for the application.func main() { // Create a value of type User and send a notification. u := user{\"Bill\", \"bill@email.com\"} sendNotification(u) // ./listing36.go:32: cannot use u (type user) as type // notifier in argument to sendNotification: // user does not implement notifier // (notify method has pointer receiver)}// sendNotification accepts values that implement the notifier// interface and sends notifications.func sendNotification(n notifier) { n.notify()} 上面的程序会编译失败，错误的原因是user类型的值并没有实现notify接口。这里涉及到了方法集的概念，方法集定义了一组关联到给定类型的值或指针的方法。定义方法时使用的接收者的类型决定了这个方法时关联到值还是关联到指针，还是两个都关联。Go语言规范里定义了方法集的规则。 12345678910Values Methods Receivers-----------------------------------------------T (t T)*T (t T) and (t *T)Methods Receivers Values-----------------------------------------------(t T) T and *T(t *T) *T 也就是说T类型的方法集只包含了值接收者声明的方法。而*T的方法集即包含了值接收者声明的方法，也包含指针接受者声明的方法。那么上面错误代码的解决方式就有两种，一种是sendNotification(&amp;n)，因为&amp;n即包含了值接收者方法，也包含了指针接收者方法。另一种是将notify修改为值接收方法。 5.4.4 多态在了解了方法集的基础上，这里给了一个展示接口的多态行为的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package mainimport ( \"fmt\")// notifier is an interface that defines notification// type behavior.type notifier interface { notify()}// user defines a user in the program.type user struct { name string email string}// notify implements the notifier interface with a pointer receiver.func (u *user) notify() { fmt.Printf(\"Sending user email to %s&lt;%s&gt;\\n\", u.name, u.email)}// admin defines a admin in the program.type admin struct { name string email string}// notify implements the notifier interface with a pointer receiver.func (a *admin) notify() { fmt.Printf(\"Sending admin email to %s&lt;%s&gt;\\n\", a.name, a.email)}// main is the entry point for the application.func main() { // Create a user value and pass it to sendNotification. bill := user{\"Bill\", \"bill@email.com\"} sendNotification(&amp;bill) // Create an admin value and pass it to sendNotification. lisa := admin{\"Lisa\", \"lisa@email.com\"} sendNotification(&amp;lisa)}// sendNotification accepts values that implement the notifier// interface and sends notifications.func sendNotification(n notifier) { n.notify()} 可以看到user和admin都实现了notify接口，对同一个行为做出了不同的表示。 5.6 嵌入类型将已有的类型直接声明在新的结构类型里被称为嵌入类型。通过嵌入类型，与内部类型相关的标识会提升到外部类型上。这些被提升的标识符和直接声明在外部类型里一样。也是外部类型的一部分。12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( \"fmt\")// user defines a user in the program.type user struct { name string email string}// notify implements a method that can be called via// a value of type user.func (u *user) notify() { fmt.Printf(\"Sending user email to %s&lt;%s&gt;\\n\", u.name, u.email)}// admin represents an admin user with privileges.type admin struct { user // Embedded Type level string}// main is the entry point for the application.func main() { // Create an admin user. ad := admin{ user: user{ name: \"john smith\", email: \"john@yahoo.com\", }, level: \"super\", } // We can access the inner type's method directly. ad.user.notify() // The inner type's method is promoted. ad.notify()} 可以看到user被嵌入到admin中，user的notify方法也被提升到了admin类型上，可以直接调用。如果admin自己也实现了notify接口，这时候user的notify方法不会被提升。 5.6 公开和未公开的标识符当一个标识符的名字以小写字母开头时，这个标识符就是未公开的，即包外的代码不可见。大写字母开头表示是公开的，对包外的代码可见。 5.7 小结使用关键字struct或者通过指定已存在的类型，可以声明用户定义的类型方法提供了一种给用户定义的类型增加行为的方式设计类型时需要确认类型的本质是原始的还是非原始的接口是声明了一组行为并支持多态的类型嵌入类型提供了扩展类型的能力，而无需使用继承标识符要么是从包里公开的，要么是在包里未公开的","link":"/2019/04/13/go_type/"},{"title":"生产者和消费者模型初探","text":"本文将尝试构造一个生产者，消费者模型，通过模型的构建，学习一下多线程编程。代码见：GitHub 1.生产者消费者模型关于生产者消费者模型的基本含义不在赘述。本实验的拟构造一个生产者，从文件中按行读取数据，放入队列中，构建十个消费者，从队列中读取数据，将数据写回文件。 1.1 starter首先我们构造一个方法，改方法负责初始化生产者和消费者线程 123456789101112131415161718192021222324252627public static void startMutiTheads(String inputPath, String outPath) { LocalDateTime startTime = LocalDateTime.now(); List&lt;Thread&gt; threads = new ArrayList&lt;&gt;(); LinkedBlockingQueue&lt;Optional&lt;String&gt;&gt; queue = new LinkedBlockingQueue&lt;&gt;(); FileUtil.clearFileContents(outPath); Producer producer = new Producer(inputPath, queue); threads.add(new Thread(producer)); Consumer consumer = new Consumer(outPath, queue); for(int i = 0; i &lt; Consumer.consumerThreadCount; i++){ Thread consumerThread = new Thread(consumer); threads.add(consumerThread); producer.addConsumer(consumer); } threads.forEach(Thread :: start); threads.forEach(thread -&gt; { try { thread.join(); } catch (InterruptedException e) { e.printStackTrace(); } }); consumer.getFileUtil().flushAndClose(); // get consumer's totalCount: logger.debug(\"Consumer's totalCount: \" + consumer.getTotalCount()); LocalDateTime endTime = LocalDateTime.now(); logger.info(String.format(\"It takes %s seconds to finish\", LocalDateTime.from(startTime).until(endTime, ChronoUnit.SECONDS))); } 函数的有两个参数，分别表表示文件的输入路径和输出路径。下面我们构建一个thread列表，该列表存储所有的线程实例。 1LinkedBlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;&gt;(); 我们选用concurrent包中的LinkedBlockingQueue队列，生产者线程将内从从文件读出放至该队列，消费者线程从改队列读出数据写回到文件。 LinkedBlockingQueue实现是线程安全的，实现了先进先出等特性，可以指定容量，也可以不指定，不指定的话，默认最大是Integer.MAX_VALUE，其中主要用到put和take方法，put方法在队列满的时候会阻塞直到有队列成员被消费，take方法在队列空的时候会阻塞，直到有队列成员被放进来。 下面我们构建了一个生产者线程，并放到theads列表中 12Producer producer = new Producer(inputPath, queue);threads.add(new Thread(producer)); 再之后构建十个消费者线程，其中Consumer.consumerThreadCount是在Counsmer中定义的一个静态变量，值为10 123456Consumer consumer = new Consumer(outPath, queue);for(int i = 0; i &lt; Consumer.consumerThreadCount; i++){ Thread consumerThread = new Thread(consumer); threads.add(consumerThread); producer.addConsumer(consumer); } 下面需要做的就是启动线程 12345678threads.forEach(Thread :: start);threads.forEach(thread -&gt; { try { thread.join(); } catch (InterruptedException e) { e.printStackTrace(); } }); thread.join()方法会一直等待，直到改线程结束。 以上就是starter需要做的。 1.2 Produce本实验只设置了一个消费者模型，基本代码如下： 1234567891011121314151617public class Producer implements Runnable { private LinkedBlockingQueue&lt;Optional&lt;String&gt;&gt; queue; private String inputFile; public Producer(LinkedBlockingQueue&lt;Optional&lt;String&gt;&gt; queue, String inputFile) { this.queue = queue; this.inputFile = inputFile; } @Override public void run() { FileUtil.readFileLineByLine(inputFile, line -&gt; { queue.add(Optional.of(line)); }); for(int i = 0; i &lt; Consumer.consumerThreadCount; i++) { queue.add(Optional.empty()); } }} producer类只是简单的重载了Runable的run方法，run方法一开始，将文件内容读入到queue队列中，这里面用到了Java8里面lanmda表达式。 line -&gt; {queue.add(Optional.of(line)} FileUtil是一个文件读取工具类，readFileLineByLine将文件按行读出到queue中,这里面同样用到了Java8中的Consumer类，关于这个类暂时按下不表，可以理解为接收一个lanmda表达式，并对每个accept的参数进行lanmda表达式的操作： 123456789101112public static void readFileLineByLine(String filePath, Consumer&lt;String&gt; consumer) { try { BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(filePath), \"Cp1252\")); String line; while ((line = br.readLine()) != null) { consumer.accept(line); } br.close(); } catch(IOException e) { e.printStackTrace(); } } 后面的for循环是将十个Optional空对象放入queue中，这里做的目的是实现对Consumer线程的结束控制，具体原理会在Consumer类中进行表述。Optional是Java8的特性，其官方解释是： 这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象 较为通俗的说法是： 如果你开发过Java程序，可能会有过这样的经历：调用某种数据结构的一个方法得到了返回值却不能直接将返回值作为参数去调用别的方法。而是首先判断这个返回值是否为null，只有在非空的前提下才能将其作为其他方法的参数。Java8中新加了Optional这个类正是为了解决这个问题。 其具体用法以后解释，在本文中可简单理解为，在生产者将所有数据都写进队列后，我们放置10个空元素进入队列，消费者可根据空元素进行停止判断。 1.3 ConsumerConsumer的构建也是比较简单， 同producer一样，继承Runable，并重写Run方法： 123456789101112131415161718192021222324252627282930313233public class Consumer implements Runnable { private AtomicInteger totalCount = new AtomicInteger(0); public static final int consumerThreadCount = 10; private FileUtil fileUtil; private final static Logger logger = Logger.getLogger(Consumer.class); private LinkedBlockingQueue&lt;Optional&lt;String&gt;&gt; queue; public Consumer(LinkedBlockingQueue&lt;Optional&lt;String&gt;&gt; queue, String outputFile) { this.queue = queue; this.fileUtil = new FileUtil(outputFile); } @Override public void run() { try { while (true) { Optional&lt;String&gt; line = queue.take(); if (!line.isPresent()) break; totalCount.incrementAndGet(); String processedLine = fileUtil.processLine(line.get()); fileUtil.appendToFile(processedLine); } } catch (InterruptedException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } public FileUtil getFileUtil() { return fileUtil; } public int getTotalCount() { return totalCount.get(); }} 主要看一下run方法，run方法使用while循环，循环读取queue中的数据。上文介绍过，queue.take()方法会一直阻塞直到队列中塞进数据。此外run方法中有使用fileUtil的processLine方法： 1234567891011121314public String processLine(String line) throws IOException { int min = 1; int max = 100; int randomMillisecconds = min + (int)(Math.random() * ((max - min) + 1)); try { Thread.sleep(randomMillisecconds); } catch (InterruptedException e) { e.printStackTrace(); } // readLineCount is not accurate in multi-thread mode readLineCount++; logger.info(String.format(\"%d lines were read.\", readLineCount)); return line; } processLine主要是对Consumer读取的行数记性计数，并在log中打印出来。为什么这样做，未在测试实验时说明。关于Consumer还有一点需要关注，我们看到Consumer的run方法体中是一个while循环，那Consumer线程什么时候会停止就变成了一个问题\u0010。按照我们设计初衷，Producer把所有的文件按行读取到queue中，Consumer回去queue中读取数据，写回到另一个文件。按照这种逻辑，如果producer读完了所有文件，Consumer也将queue中的所有数据写回文件，此时Consumer就应该停止了。 1if (!line.isPresent()) break; 这段代码就是负责停止Consumer线程的。记得我们在Producer,当所有数据都读取到queue中时，会在queue中塞入十个optional.empty变量，那如果在Consumer中queue.take()返回的是optinal.empty，就说明queue已经无数据了，当前Consumer就可以停止了。关于如何在循环中停止线程，还有很多方法，待后面有时间再做解析。以上就是Consumer类的构造。 2.实验实验代码： 12345@Test public void multiThreadTest() throws IOException { starter.startMultiThreadTask(inputFile, outputFile); Assert.assertTrue(isFileDataSame(inputFile, outputFile)); } 实验结果,实验结果会打印总耗时以及两个count变量： 12345672017-03-01 13:52:04 INFO FileUtil:118 - 2954 lines were read.2017-03-01 13:52:04 INFO FileUtil:118 - 2955 lines were read.2017-03-01 13:52:04 INFO FileUtil:118 - 2956 lines were read.2017-03-01 13:52:04 INFO FileUtil:118 - 2957 lines were read.2017-03-01 13:52:04 INFO FileUtil:118 - 2958 lines were read.2017-03-01 13:52:04 DEBUG Starter:50 - Consumer&apos;s totalCount: 30002017-03-01 13:52:04 INFO Starter:53 - It takes 16 seconds to finish 解释一下，诸如2958 lines were read，是从FileUtil工具类processLine函数中打印出的readLine变量，其代表意义是线程Consumer线程从queue中读取了多少行。Consumer’s totalCount: 3000 是直接打印的Consumer全局变totalCount，其代表意义同样是十个线程总共从queue中读取了多少行，按道理来说，这两个值是应该相同的，然后结果明显不一致。 从代码我们可以看到readLine是一个int型变量，而totalCount是一个AtomicInteger变量，很显然问题出在了这里。我们知道Java中++这种操作是线程不安全的，而readLineCount是个全局变量，所以如果多个线程同事在执行++操作时，就会产生totalCount的值不一致的问题，解决方法可以粗暴的在processLine中加上synchronized关键字： 12345678910111213141516public String processLine(String line) throws IOException { synchronized (this) { int min = 1; int max = 100; int randomMillisecconds = min + (int) (Math.random() * ((max - min) + 1)); try { Thread.sleep(randomMillisecconds); } catch (InterruptedException e) { e.printStackTrace(); } // readLineCount is not accurate in multi-thread mode readLineCount++; logger.info(String.format(\"%d lines were read.\", readLineCount)); return line; } } 我们再次运行测试脚本： 1234567892017-03-01 14:09:34 INFO FileUtil:119 - 2994 lines were read.2017-03-01 14:09:34 INFO FileUtil:119 - 2995 lines were read.2017-03-01 14:09:34 INFO FileUtil:119 - 2996 lines were read.2017-03-01 14:09:34 INFO FileUtil:119 - 2997 lines were read.2017-03-01 14:09:35 INFO FileUtil:119 - 2998 lines were read.2017-03-01 14:09:35 INFO FileUtil:119 - 2999 lines were read.2017-03-01 14:09:35 INFO FileUtil:119 - 3000 lines were read.2017-03-01 14:09:35 DEBUG Starter:50 - Consumer&apos;s totalCount: 30002017-03-01 14:09:35 INFO Starter:53 - It takes 160 seconds to finish 可以看到两个变量的值相等了，说明这种方法可行，但是花费的时间确从16s到了160s，说明synchronized关键字极大的增加了时间的消耗，我们分析processLine方法，其实问题只出在readLineCount上，concurrent包中提供了AtomicInteger变量，它实现了对int变量的封装，实现了对自增操作的原子性。为此我们将readLineCount定义为： 1private AtomicInteger readLineCount = new AtomicInteger(0); processLine函数变为： 1234567891011121314public String processLine(String line) throws IOException { int min = 1; int max = 100; int randomMillisecconds = min + (int) (Math.random() * ((max - min) + 1)); try { Thread.sleep(randomMillisecconds); } catch (InterruptedException e) { e.printStackTrace(); } // readLineCount is not accurate in multi-thread mode readLineCount.incrementAndGet(); logger.info(String.format(\"%d lines were read.\", readLineCount.get())); return line; } 再次运行测试代码： 1234567892017-03-01 14:18:23 INFO FileUtil:120 - 2994 lines were read.2017-03-01 14:18:23 INFO FileUtil:120 - 2995 lines were read.2017-03-01 14:18:23 INFO FileUtil:120 - 2996 lines were read.2017-03-01 14:18:23 INFO FileUtil:120 - 2997 lines were read.2017-03-01 14:18:23 INFO FileUtil:120 - 2998 lines were read.2017-03-01 14:18:23 INFO FileUtil:120 - 2999 lines were read.2017-03-01 14:18:23 INFO FileUtil:120 - 3000 lines were read.2017-03-01 14:18:23 DEBUG Starter:50 - Consumer&apos;s totalCount: 30002017-03-01 14:18:23 INFO Starter:53 - It takes 15 seconds to finish 可以看到，两个变量值相等了，然后耗时只用了15s。从以上实验可以看到使用concurrent包中的变量和方法，比简单粗暴的使用synchronized这种方法在耗时方便具有很大的优势。 3.总结以上我们初步试验了一个简单的生产者消费者模型，使用了Cocurrent包中的一些方法。时间比较急，先写这么多，后续有内容再行添加。","link":"/2017/12/04/proandconsu/"},{"title":"深入理解kafka学习笔记","text":"kafka起初由LinkedIn公司采用Scala语言开发的一个多分区、多副本基于zookeeper协调的分布式消息系统，它以高吞吐、可持久化、可水平扩展、支持流数据处理等多种特性而被广泛应用。本文通过生产者、消费者、主题分区、日志存储等几个方面最kafka的基本用法和原理进行阐述，旨在以后方面回顾。文中大部分内容提取自《深入理解Kakfa 核心设计与实践原理》。 1. 初识kafka1.1 基本概念 生产者 消费者 Broker，可以简单看做是一个独立的kafka服务节点或者服务实例。 主题，kafka中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题，消费者订阅主题并进行消费 分区，一个分区只属于单个主题，每个主题可以包含多个分区，分区可以分布在不同的broker上，同一主题下不同分区包含的消息是不同的 分区在存储层面可以看做是一个可追加的日志本间。每条消息被发送到broker之前，会根据分区规则选择存储到哪个具体的分区。分区规则如果设置的合理，那么消息会被均匀的分配到不同的分区中。 kafka为分区引入了多副本(Replica)机制，通过增加副本的数量可以提升容灾能力。副本之间是一主多从的关系，leader副本负责处理读写请求，follower副本负责和leader副本消息同步。副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。kafka通过多副本机制实现了故障的自动转移，当kafka集群中某个broker失效时仍能保证服务可用。 分区中所有的副本统称AR(Assigned Replicas), 所有与leader副本保持一定程度同步的副本(包括leader副本在内)组成ISR(In-Sync Replicas)。消息会先发送到leader副本，然后follower副本从leader副本中拉取消息进行同步。 LEO: 高水位，副本最大位移 HW: 低水位，ISR列表中最小的LEO，是消费者可消费的最大位移 2. 生产者最基本的生产者代码 1234567891011Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"acks\", \"all\");props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);for (int i = 0; i &lt; 100; i++) producer.send(new ProducerRecord&lt;String, String&gt;(\"my-topic\", Integer.toString(i), Integer.toString(i)));producer.close(); bootstrap.servers: 设置集群broker地址清单，具体格式：host1:port1, host2:port2 可以设置一个或者多个，不需要设置所有的broker地址，生产者可以从给定的broker里面查到其他的broker信息，不过建议最少设置两个以上，其中一个宕机时，生产者还可以连到集群上 key.serializer、value.serializer用来序列化ProducerRecord asks: 这个参数用来指定分区中必须要有多少副本收到这条消息，之后生产者才会认为这条消息是成功写入的, asks的设置会影响吞吐量 asks == 1. 即只要leader副本成功写入消息，那么生产端就会收到服务端的成功响应 asks == 0. 生产者不需要等待任何服务服务端的响应 asks == -1 or asks == all. 需要等待ISR列表中的所有副本都成功写入，服务器端才能响应成功 发送数据三种模式，发后即忘，同步和异步 同步： 123456try{ Future&lt;RecordMetadata&gt; future = producer.send(record); RecordMetadata metadata = future.get()} catch(...) { ...} 异步123456producer.send(record, new Callback(){ @Override public void onCompletion(RecordMetadata metadata, Exception exception){ ... }}); 消息缓存到batch，分批次发送，减少网络传输的资源消耗send时，生产者并不会立即发送记录到broker，会先对记录做缓存。从图中可以看出，send会先发消息放到一个batch中，当一个batch满了之后，再申请一个新的batch填充，kafka消费端维护了一个batch池，以便batch可以循环利用。此外消费者客户端会有一个send线程，实时去队列中获取已经满了的batch，发送到broker 3. 消费者kafka中的消费是基于拉取的,消费者会重复的调用poll方法拉取数据 123456789101112131415Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"group.id\", \"test\");props.put(\"enable.auto.commit\", \"true\");props.put(\"auto.commit.interval.ms\", \"1000\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(Arrays.asList(\"foo\", \"bar\"));while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());} group.id: 消费者隶属的消费组名称, 消费组可以包含多个消费者，每个分片只能被一个消费组中的一个消费者消费 enable.auto.commit: 消费者消费完数据，需要告诉服务端消费的offset，以便下次poll数据时，服务端知道从哪里开始下发数据, 这个字段告诉消费者自动提交offset，消费者后台线程会定期自动提交offset，周期可以通过auto.commit.interval.ms设置，默认5s 但是自动提交会产生漏数据或者重复消费的问题。加入消费者一次poll到10条数据，在消费到第五条的时候，后台线程提交了offset，提交后消费者线程挂掉，此时服务端会认为消费者已经将这10条数据消费，消费者重启，再poll数据时，会直接拉取10条后的数据。不过因为kafka数据存储实在磁盘上的，会周期性的持久化，如果知道漏数据的offset，可以通过offset重新拉取数据如果消费者在消费完数据后，提交位移前挂掉，会发生重复消费的问题。如果消费者业务是幂等的，重复消费不会产生太多问题，否则不太乐观kafka同时提供了手动提交位移的api1234567891011121314//一次poll消费完统一提交位移Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"group.id\", \"test\");props.put(\"enable.auto.commit\", \"false\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(Arrays.asList(\"foo\", \"bar\"));while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); insertIntoDb(records); consumer.commitSync();} 12345678910111213141516//每条记录消费完就提交try { while(running) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE); for (TopicPartition partition : records.partitions()) { List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) { System.out.println(record.offset() + \": \" + record.value()); } long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1))); } }} finally { consumer.close();} 4. 主题分区5. 日志存储kafka以log日志的形式将数据存储到磁盘上，每个partition可以看做是一个整体的log文件，但为了不使单个log文件太大，kafka引入了日志分段(LogSegment)，将log日志切分成多个LogSegment，相当于被平分为多个较小的文件 index 中存储了索引以及物理偏移量。 log 存储了消息的内容。索引文件的元数据执行对应数据文件中message 的物理偏移地址。举个简单的案例来说，以[4053,80899]为例，在 log 文件中，对应的是第 4053 条记录，物理偏移量（position）为 80899. position 是ByteBuffer 的指针位置 查找算法 根据offset的值，查找segment段中的index 索引文件。由于索引文件命名是以上一个文件的最后一个offset 进行命名的，所以，使用二分查找算法能够根据offset 快速定位到指定的索引文件。 找到索引文件后，根据 offset 进行定位，找到索引文件中的符合范围的索引。(跳表) 得到 position 以后，再到对应的 log 文件中，从 position出开始查找 offset 对应的消息，将每条消息的 offset 与目标 offset 进行比较，直到找到消息 顺序写，页缓存和零拷贝 顺序写: 磁盘的顺序写速度要远大于随机写，kafka在设计时采用了文件追加的方式写入消息，即只能在日志文件的尾部追加新的消息并且也不允许修改已经写入的消息，这种方式属于典型的顺序写盘的操作 页缓存: kafka采用mmap直接操作页缓存pagecache的方式，最终数据的读写基本都是操作内存，脏页数据由操作系统统一刷到磁盘， kafka提供了参数可以强制刷盘。页缓存存在的风险是，如果机器断电，页缓存中脏页数据没有刷到磁盘，会导致丢数据。但这个问题仅在机器断电时发生，多副本机制可以保障数据的可靠性。 Linux会使用磁盘的一部分作为swap分区，这样可以进行进程的调度，把当前非活跃的进程调入swap分区，以此把内存空出来让给活跃的进程。但是对kafka来说，如果大量的页缓存被置换到swap区，会极大的降低kafka的性能，但是如果不设置swap分区，当内存不够用时，会导致OOM的发生。建议swap设置的小一些。 使用页缓存使同时可以避免在JVM内部缓存数据，降低内存消耗 零拷贝: 在从磁盘读数据发送给消费者时，传统的做法被将数据从磁盘拷贝的内核空间，再由内核拷贝到用户空间，然后由用户控件拷贝到内核态的socket buffer中，最后内核态的buffer中拷贝到网卡，这中间需要四次复制过程，零拷贝技术将拷贝次数降低到两次","link":"/2019/05/05/kafka/"},{"title":"一个线程安全的单例模式","text":"单例模式的一般构造方法 123456789101112131415public class SingletonConsumer { private final static Logger logger = Logger.getLogger(SingletonConsumer.class); private static SingletonConsumer instance; private SingletonConsumer() { } public SingletonConsumer getInstance() { if (instance == null) { logger.debug(\"instance is null, trying to instantiate a new one\"); instance = new SingletonConsumer(); } else { logger.debug(\"instance is not null, return the already-instantiated one\"); } return instance; }} 以上这种构造方法在单线程下运行是安全的，但是如果放到多线程下，则会出现各种各样的问题，为此我们设计一个实验来验证多线程下，以上方法会出现什么问题。 Experiment实验中我们设置10个线程去创建SingletonConsumer实例，最后验证到底创建了多少个实例。 12345678910111213@Test public void singletonConsumerTest() throws InterruptedException { ExecutorService executors = Executors.newFixedThreadPool(10); Set&lt;SingletonConsumer&gt; set = new HashSet&lt;&gt;(); for(int i = 0; i &lt; 10; i++){ executors.execute( () -&gt; set.add(SingletonConsumer.getInstance()) ); } executors.shutdown(); executors.awaitTermination(1, TimeUnit.HOURS); Assert.assertEquals(10, set.size()); } 运行测试，输出结果如下 12345678910112017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new one2017-02-26 13:50:52 DEBUG SingletonConsumer:20 - instance is null, trying to instantiate a new oneset size:10 会发现此时实际上构造了是个SingletonConsumer实例，那怎么才能构造线程安全的单例模式？首先想到的方法是将getInstance的代码用synchronized包起来，这样就能够保证getInstance方法每次只能有一个线程访问到，于是代码就变成了下面的样子 1234567891011public static SingletonConsumer getInstance() { synchronized (SingletonConsumer.class) { if (instance == null) { logger.debug(\"instance is null, trying to instantiate a new one\"); instance = new SingletonConsumer(); } else { logger.debug(\"instance is not null, return the already-instantiated one\"); } return instance; } } 我们再次运行测试脚本 12345678910111213142017-02-26 14:01:12 DEBUG SingletonConsumer:21 - instance is null, trying to instantiate a new one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated one2017-02-26 14:01:12 DEBUG SingletonConsumer:24 - instance is not null, return the already-instantiated onejava.lang.AssertionError: Expected :10Actual :1 &lt;Click to see difference&gt; 此时发现，只初始化了一个SingletonConsumer实例，说明这种方法是work的。 但是仔细去想一想，上面的方法是有效率问题的。假设有一个线程A正在synchronized块中判断instance是否为null，此时其他线程只能等待线程A判断完毕才可以再去判断。仔细想想，instance是否为空，其实是可以多个线程同时去判断的，因此我们将代码修改成一下形式： 123456789if (instance == null) { synchronized (SingletonConsumer.class) { logger.debug(\"instance is null, trying to instantiate a new one\"); instance = new SingletonConsumer(); } } else { logger.debug(\"instance is not null, return the already-instantiated one\"); } return instance; 上面的代码中，我们将instance是否为空的判断移到了同步块的外面。那这种方法是否是线程安全的呢，再次运行测试脚本，观察结果： 12345678910112017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new one2017-02-26 14:14:18 DEBUG SingletonConsumer:28 - instance is null, trying to instantiate a new oneset size:10 通过结果发现，依然实例化了10个SingeltonCoumser。考虑一种情况，初始install为空，线程A判断完instance是否为空，发现instance为null，刚好线程A的时间片用完，轮到线程B去判断instance是否为空，线程B发现instance也是null，此时时间片又回到了线程A的手中，线程A去创建SingletonConsumer对象，创建完成，线程B去创建对象，这样下去，就造成了上述实验的现象，因此，未解决上面的问题，需要带同步块中同样去判断instance是否为null。最后的代码如下： 12345678910111213141516public static SingletonConsumer getInstance() { if (instance == null) { synchronized (SingletonConsumer.class) { if (instance == null) { logger.debug(\"instance is null, trying to instantiate a new one\"); instance = new SingletonConsumer(); } else { logger.debug(\"instance is not null, return the already-instantiated one \"); } } } else { logger.debug(\"instance is not null, return the already-instantiated one\"); } return instance; } 最有还有一点要注意，由于JVM会对代码进行优化，所以代码的执行顺序在真运行的时候会发生变化，会导致赋值操作编程不可见的，因此才进行赋值操作时，instance有可能只拿到一个为完全初始化的实例，这样会导致一些错误。 instance = new SingletonConsumer(); 解决办法是将instance生命为volatile的，volatile关键词可以保证可见性和有序性，其具体内容待下次再表。 Summary总之，一个线程安全的单例模式需要注意以下3点： 1.getInstance的需要用synchronized关键词 2.为提高效率，instance是否为空可提到同步块以外，但内层的判断依然要保留 3.instance需要声明为volatile 代码见：GitHub","link":"/2017/12/05/safe-singleton/"}],"tags":[{"name":"闭包","slug":"闭包","link":"/tags/闭包/"},{"name":"Golang","slug":"Golang","link":"/tags/Golang/"},{"name":"装饰器","slug":"装饰器","link":"/tags/装饰器/"},{"name":"多线程","slug":"多线程","link":"/tags/多线程/"},{"name":"生产者消费者模型","slug":"生产者消费者模型","link":"/tags/生产者消费者模型/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"单例模式","slug":"单例模式","link":"/tags/单例模式/"},{"name":"线程安全","slug":"线程安全","link":"/tags/线程安全/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Go In Action","slug":"Go-In-Action","link":"/categories/Go-In-Action/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"kafka","slug":"kafka","link":"/categories/kafka/"}]}